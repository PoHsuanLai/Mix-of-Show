# Base configuration for Anything-v4 model training

models:
  pretrained_path: experiments/pretrained_models/anything-v4.0
  enable_edlora: true  # true means ED-LoRA, false means vanilla LoRA
  finetune_cfg:
    text_embedding:
      enable_tuning: true
      lr: !!float 1e-3
    text_encoder:
      enable_tuning: true
      lora_cfg:
        rank: 4
        alpha: 1.0
        where: CLIPAttention
      lr: !!float 1e-5
    unet:
      enable_tuning: true
      lora_cfg:
        rank: 4
        alpha: 1.0
        where: Attention
      lr: !!float 1e-4
  noise_offset: 0.01
  attn_reg_weight: 0.01
  reg_full_identity: false
  use_mask_loss: false
  gradient_checkpoint: false
  enable_xformers: true

# path
path:
  pretrain_network: ~
  models: experiments/composed_edlora/anythingv4  # Where to save models

# training settings
train:
  optim_g:
    type: AdamW
    lr: !!float 0.0  # no use since we define different component lr in model
    weight_decay: 0.01
    betas: [0.9, 0.999]  # align with taming

  scheduler: linear
  emb_norm_threshold: !!float 5.5e-1
  total_iter: 500  # Total training iterations
  gradient_accumulation_steps: 1

# validation settings
val:
  val_during_save: true
  compose_visualize: true
  alpha_list: [0, 0.7, 1.0]  # 0 means only visualize embedding (without lora weight)
  sample:
    num_inference_steps: 50
    guidance_scale: 7.5

# logging settings
logger:
  print_freq: 10
  save_checkpoint_freq: !!float 100

# mixed precision settings
mixed_precision: "fp16" 